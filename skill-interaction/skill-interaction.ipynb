{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faddfe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import speech_recognition as sr\n",
    "\n",
    "import subprocess\n",
    "import utilities\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pyaudio\n",
    "import math\n",
    "import struct\n",
    "import wave\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA_DIR\"\n",
    "SKILLS_ADDR = os.path.join(DATA_DIR,'subgrouped_skills.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ed57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = 'PERSONA_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a621e5",
   "metadata": {},
   "source": [
    "### Listen and record Alexa responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = 15\n",
    "\n",
    "SHORT_NORMALIZE = (1.0/32768.0)\n",
    "chunk = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "swidth = 2\n",
    "\n",
    "TIMEOUT_LENGTH = 5\n",
    "\n",
    "class Recorder:\n",
    "\n",
    "    @staticmethod\n",
    "    def rms(frame):\n",
    "        count = len(frame) / swidth\n",
    "        format = \"%dh\" % (count)\n",
    "        shorts = struct.unpack(format, frame)\n",
    "\n",
    "        sum_squares = 0.0\n",
    "        for sample in shorts:\n",
    "            n = sample * SHORT_NORMALIZE\n",
    "            sum_squares += n * n\n",
    "        rms = math.pow(sum_squares / count, 0.5)\n",
    "\n",
    "        return rms * 1000\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(format=FORMAT,\n",
    "                                  channels=CHANNELS,\n",
    "                                  rate=RATE,\n",
    "                                  input=True,\n",
    "                                  output=True,\n",
    "                                  frames_per_buffer=chunk)\n",
    "\n",
    "    def record(self, file_name):\n",
    "        print('Noise detected, recording now')\n",
    "        rec = []\n",
    "        current = time.time()\n",
    "        end = time.time() + TIMEOUT_LENGTH\n",
    "        \n",
    "        default_end = time.time() + 30\n",
    "        force_stop = False\n",
    "        \n",
    "        while current <= end:\n",
    "            data = self.stream.read(chunk)\n",
    "\n",
    "            if self.rms(data) >= Threshold: \n",
    "                end = time.time() + TIMEOUT_LENGTH\n",
    "\n",
    "            current = time.time()\n",
    "            rec.append(data)\n",
    "            \n",
    "            # stop recording after 30 seconds\n",
    "            if current > default_end:\n",
    "                force_stop = True\n",
    "                break \n",
    "                \n",
    "        self.write(b''.join(rec), file_name)\n",
    "        \n",
    "        return force_stop\n",
    "\n",
    "        \n",
    "    def write(self, recording, file_name):\n",
    "        time_1 = os.path.getmtime(file_name + '-1.wav')\n",
    "        time_2 = os.path.getmtime(file_name + '-2.wav')\n",
    "\n",
    "        if time_1 > time_2:\n",
    "            file_name = file_name + '-2.wav'\n",
    "        else:\n",
    "            file_name = file_name + '-1.wav'\n",
    "\n",
    "        wf = wave.open(file_name, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(self.p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(recording)\n",
    "        wf.close()\n",
    "        print('Written to file: {}'.format(file_name))\n",
    "        print('Listening again')\n",
    "\n",
    "\n",
    "    def listen(self, file_name):\n",
    "        count = 0\n",
    "        speak_counter = 0\n",
    "        force_stop = False\n",
    "        \n",
    "        while True:\n",
    "            input = self.stream.read(chunk)\n",
    "            rms_val = self.rms(input)\n",
    "            \n",
    "            if rms_val >= Threshold:\n",
    "                count = 0\n",
    "\n",
    "                force_stop = self.record(file_name)\n",
    "                \n",
    "                if force_stop:\n",
    "                    break\n",
    "            \n",
    "            elif rms_val < Threshold:\n",
    "                count += 1\n",
    "                if count > 30:\n",
    "                    break\n",
    "          \n",
    "        return force_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f9504",
   "metadata": {},
   "source": [
    "### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eba1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(file_name):\n",
    "    responses = []\n",
    "    responses.append(get_last_response(file_name + '-1.wav'))\n",
    "    responses.append(get_last_response(file_name + '-2.wav'))\n",
    "    return list(filter(None, responses))\n",
    "    \n",
    "\n",
    "def get_last_response(last_recording):\n",
    "    speech_to_text = sr.Recognizer()\n",
    "    text = ''\n",
    "    \n",
    "    try:\n",
    "        with sr.AudioFile(last_recording) as source:\n",
    "            # listen for the data (load audio to memory)\n",
    "            audio_data = speech_to_text.record(source)\n",
    "            # recognize (convert from speech to text)\n",
    "            text = speech_to_text.recognize_google(audio_data, language = \"en\")\n",
    "            text = text['alternative'][0]['transcript']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(text) == 0:\n",
    "        return ''\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def file_clean_up(LAST_RECORDING, CURRENT_RECORDING):\n",
    "    os.remove(LAST_RECORDING + '-1.wav')\n",
    "    os.remove(LAST_RECORDING + '-2.wav')\n",
    "    \n",
    "    os.rename(CURRENT_RECORDING + '-1.wav', LAST_RECORDING + '-1.wav')\n",
    "    os.rename(CURRENT_RECORDING + '-2.wav', LAST_RECORDING + '-2.wav')\n",
    "    \n",
    "    utterance_wav = gTTS(text='None', lang='en', slow=False)\n",
    "    utterance_wav.save(CURRENT_RECORDING + '-1.wav')\n",
    "    utterance_wav.save(CURRENT_RECORDING + '-2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02fd128",
   "metadata": {},
   "source": [
    "### Interaction logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utterances(skills_json, no_skills_to_interact = 50):\n",
    "    skills_utterances = {}\n",
    "    total_extracted = 0 \n",
    "    \n",
    "    for skill in skills_json:\n",
    "        skills_utterances[skill] = []\n",
    "\n",
    "        for command in skills_json[skill]['Sample_Invocation_Utterances']:\n",
    "            if command.strip() == '':\n",
    "                continue\n",
    "            command = command.replace('”', '').replace('“', '').replace('.', '').strip()\n",
    "            \n",
    "            if not command.lower().startswith('alexa'):\n",
    "                command = 'Alexa, ' + command\n",
    "            \n",
    "            skills_utterances[skill].append(command)\n",
    "\n",
    "        skill_desc = skills_json[skill]['Skill_description'].\\\n",
    "                        replace('”', '\"').replace('“', '\"').replace('‘','').replace('’','')\n",
    "        more_commands = re.findall(r'\"([^\"]*)\"', skill_desc)\n",
    "\n",
    "        more_commands_counter = 0\n",
    "        for command in more_commands:\n",
    "            if command.lower().startswith('alexa') and len(command.strip()) > len('alexa'):\n",
    "                if any(x in command for x in ['*','#','(',')','[',']','{','}','<','>']): \n",
    "                    continue\n",
    "                \n",
    "                if command.strip().count(' ') > 1 and command.replace('.', '').strip() not in skills_utterances[skill]:  \n",
    "                    skills_utterances[skill].append(command.replace('.', '').strip())\n",
    "                    \n",
    "                    # Only recording 5 occurrences at max from description.\n",
    "                    more_commands_counter += 1\n",
    "                    if more_commands_counter >= 5:\n",
    "                        break\n",
    "\n",
    "        total_extracted += 1\n",
    "        if total_extracted >= no_skills_to_interact:\n",
    "            break\n",
    "            \n",
    "    return skills_utterances\n",
    "    \n",
    "\n",
    "def play_utterances(skills_utterances, CURRENT_UTTERANCE, LAST_RECORDING, CURRENT_RECORDING, ALEXA_STOP):    \n",
    "    \n",
    "    # To update progress bar\n",
    "    total_utterances = 0\n",
    "    for skill in skills_utterances:\n",
    "        for utterance in skills_utterances[skill]:\n",
    "            total_utterances += 1\n",
    "        \n",
    "    pbar = tqdm(total=total_utterances, position=0, leave=True)\n",
    "    \n",
    "    for skill in skills_utterances:\n",
    "        for utterance in skills_utterances[skill]:\n",
    "\n",
    "            utterance_wav = gTTS(text=utterance, lang='en', slow=False)\n",
    "            utterance_wav.save(CURRENT_UTTERANCE + '.wav')\n",
    "            subprocess.call(['afplay', CURRENT_UTTERANCE + '.wav'])\n",
    "            \n",
    "            last_responses = get_responses(LAST_RECORDING)\n",
    "            \n",
    "            # continue to next skill after listening \n",
    "            record_response = Recorder()\n",
    "            force_stop = record_response.listen(CURRENT_RECORDING)\n",
    "            del record_response\n",
    "            \n",
    "            if force_stop:\n",
    "                subprocess.call(['afplay', ALEXA_STOP + '.wav'])\n",
    "            \n",
    "            else:\n",
    "                current_responses = get_responses(CURRENT_RECORDING)\n",
    "                print('LAST:', last_responses)\n",
    "                print('CURRENT:', current_responses)\n",
    "                \n",
    "                if any(x in current_responses for x in last_responses): \n",
    "                    subprocess.call(['afplay', ALEXA_STOP + '.wav'])\n",
    "            \n",
    "            file_clean_up(LAST_RECORDING, CURRENT_RECORDING)\n",
    "            time.sleep(2)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e13bb",
   "metadata": {},
   "source": [
    "### Initialize default file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_RECORDING = 'last-recording'\n",
    "CURRENT_RECORDING = 'current-recording'\n",
    "CURRENT_UTTERANCE = 'current-utterance'\n",
    "ALEXA_STOP = 'alexa-stop'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d3e08",
   "metadata": {},
   "source": [
    "### Write helper audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_wav = gTTS(text='Alexa, stop', lang='en', slow=False)\n",
    "utterance_wav.save(ALEXA_STOP + '.wav')\n",
    "\n",
    "utterance_wav = gTTS(text='None', lang='en', slow=False)\n",
    "utterance_wav.save(LAST_RECORDING + '-1.wav')\n",
    "utterance_wav.save(LAST_RECORDING + '-2.wav')\n",
    "utterance_wav.save(CURRENT_RECORDING + '-1.wav')\n",
    "utterance_wav.save(CURRENT_RECORDING + '-2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fb1a1",
   "metadata": {},
   "source": [
    "### Extract utterances from samples utterances and skill description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = utilities.read_json(SKILLS_ADDR)\n",
    "all_skills = all_skills[PERSONA]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cd382",
   "metadata": {},
   "source": [
    "### Play utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d39cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skills_utterances = get_utterances(all_skills, no_skills_to_interact=50) \n",
    "play_utterances(skills_utterances, CURRENT_UTTERANCE, LAST_RECORDING, CURRENT_RECORDING, ALEXA_STOP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
